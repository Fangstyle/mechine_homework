{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T03:47:23.208982Z",
     "start_time": "2025-01-07T03:47:23.012605Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import *\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# 判断可用的设备是 CPU 还是 GPU，并将模型移动到对应的计算资源设备上\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T03:47:29.576100Z",
     "start_time": "2025-01-07T03:47:24.887273Z"
    }
   },
   "source": [
    "#文本分类\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "result = classifier(\"This is a great movie. I enjoyed it a lot!\")[0]\n",
    "print(result)\n",
    "\n",
    "result = classifier(\"This movie is so bad, I almost fell asleep.\")[0]\n",
    "print(result)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'POSITIVE', 'score': 0.9998773336410522}\n",
      "{'label': 'NEGATIVE', 'score': 0.9997857213020325}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集查看"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T03:47:48.849017Z",
     "start_time": "2025-01-07T03:47:34.776324Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb_dataset = load_dataset('imdb')# 加载imdb数据集\n",
    "print(imdb_dataset['train'][0]) # 查看第一条数据\n",
    "print(imdb_dataset['train'][-1]) # 查看最后一条数据"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n",
      "{'text': 'The story centers around Barry McKenzie who must go to England if he wishes to claim his inheritance. Being about the grossest Aussie shearer ever to set foot outside this great Nation of ours there is something of a culture clash and much fun and games ensue. The songs of Barry McKenzie(Barry Crocker) are highlights.', 'label': 1}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T03:48:06.205085Z",
     "start_time": "2025-01-07T03:47:54.085601Z"
    }
   },
   "source": [
    "#定义数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.dataset = load_dataset(path='imdb', split=split)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text = self.dataset[i]['text']\n",
    "        label = self.dataset[i]['label']\n",
    "        return text, label\n",
    "\n",
    "\n",
    "train_dataset = Dataset('train')\n",
    "test_dataset = Dataset('test')"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T03:48:07.747715Z",
     "start_time": "2025-01-07T03:48:07.740425Z"
    }
   },
   "source": [
    "print(len(train_dataset), len(test_dataset))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词元化"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T03:48:10.244494Z",
     "start_time": "2025-01-07T03:48:09.908274Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#加载Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "tokenizer"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T03:48:11.289638Z",
     "start_time": "2025-01-07T03:48:11.269957Z"
    }
   },
   "source": [
    "# 数据集处理函数\n",
    "def collate_fn(data):\n",
    "    sents = [i[0] for i in data]\n",
    "    labels = [i[1] for i in data]\n",
    "\n",
    "    #编码\n",
    "    data = tokenizer.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                                   truncation=True,\n",
    "                                   padding='max_length',\n",
    "                                   max_length=500,\n",
    "                                   return_tensors='pt',\n",
    "                                   return_length=True)\n",
    "\n",
    "    #input_ids:编码之后的数字\n",
    "    #attention_mask:是补零的位置是0,其他位置是1\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, labels\n",
    "\n",
    "\n",
    "#定义数据加载器\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                     batch_size=32,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                              batch_size=32,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True)\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T03:48:13.333744Z",
     "start_time": "2025-01-07T03:48:12.659576Z"
    }
   },
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "#加载预训练bert模型\n",
    "pretrained = BertModel.from_pretrained('bert-base-cased').to(device)\n",
    "\n",
    "#不训练,不需要计算梯度\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad_(False)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T03:48:14.858551Z",
     "start_time": "2025-01-07T03:48:14.844967Z"
    }
   },
   "source": [
    "# 定义下游任务模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        with torch.no_grad():\n",
    "            out = pretrained(input_ids=input_ids,\n",
    "                       attention_mask=attention_mask,\n",
    "                       token_type_ids=token_type_ids)\n",
    "        out = self.fc(out.last_hidden_state[:, 0]) # 最后一层隐藏层作为输入\n",
    "        out = out.softmax(dim=1)\n",
    "        return out\n",
    "\n",
    "model = Model().to(device)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T03:48:16.020688Z",
     "start_time": "2025-01-07T03:48:16.007268Z"
    }
   },
   "source": [
    "# 定义训练器\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, valid_loader):\n",
    "        # 初始化训练数据集和验证数据集的dataloader\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        \n",
    "        self.device = device\n",
    "        self.model = model.to(self.device)\n",
    "        \n",
    "        # 定义优化器、损失函数和学习率调度器\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=0.001)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.scheduler = optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=0.95)\n",
    "        \n",
    "        # 记录训练过程中的损失和验证过程中的准确率\n",
    "        self.train_losses = []\n",
    "        self.val_accuracy = []\n",
    "    \n",
    "    def train(self, num_epochs):\n",
    "        # tqdm用于显示进度条并评估任务时间开销\n",
    "        for epoch in tqdm(range(num_epochs), file=sys.stdout):\n",
    "            # 记录损失值\n",
    "            total_loss = 0\n",
    "\n",
    "            # 批量训练\n",
    "            self.model.train()\n",
    "            \n",
    "            for input_ids, attention_mask, token_type_ids, labels in train_loader:\n",
    "                # 预测、损失函数、反向传播\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(input_ids=input_ids.to(self.device), attention_mask=attention_mask.to(self.device), token_type_ids=token_type_ids.to(self.device)).to(self.device)\n",
    "                loss = self.criterion(outputs, labels.to(self.device))\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            # 更新优化器的学习率\n",
    "            self.scheduler.step()\n",
    "            # 计算验证集的准确率\n",
    "            accuracy = self.validate()\n",
    "            \n",
    "            # 记录训练集损失和验证集准确率\n",
    "            self.train_losses.append(total_loss)\n",
    "            self.val_accuracy.append(accuracy)\n",
    "            \n",
    "            # 打印中间值\n",
    "            tqdm.write(\"Epoch: {0} Loss: {1} Acc: {2}\".format(\n",
    "                epoch, self.train_losses[-1], self.val_accuracy[-1]))\n",
    "    \n",
    "    def validate(self):\n",
    "        # 测试模型，不计算梯度\n",
    "        self.model.eval()\n",
    "        \n",
    "        # 记录总数和预测正确数\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for input_ids, attention_mask, token_type_ids, labels in self.valid_loader:\n",
    "                outputs = self.model(input_ids=input_ids.to(self.device), attention_mask=attention_mask.to(self.device), token_type_ids=token_type_ids.to(self.device)).to(self.device)\n",
    "                # 记录验证集总数和预测正确数\n",
    "                total += labels.size(0)\n",
    "                correct += (outputs.argmax(1) == labels.to(self.device)).sum().item()\n",
    "        \n",
    "        # 返回准确率\n",
    "        accuracy = correct / total\n",
    "        return accuracy"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练和验证"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-07T03:48:23.220701Z"
    }
   },
   "source": [
    "# 创建一个 Trainer 类的实例\n",
    "trainer = Trainer(model, train_loader, test_loader)\n",
    "# 训练模型，迭代 30 个周期\n",
    "trainer.train(num_epochs = 10)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T05:50:28.771256Z",
     "start_time": "2024-11-06T05:50:28.770970Z"
    }
   },
   "source": [
    "# 使用Matplotlib绘制损失曲线图\n",
    "plt.plot(trainer.train_losses, label='loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T05:50:28.773484Z",
     "start_time": "2024-11-06T05:50:28.772899Z"
    }
   },
   "source": [
    "# 使用Matplotlib绘制准确率曲线图\n",
    "plt.plot(trainer.val_accuracy, label='accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 直接finetune"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T05:50:28.774903Z",
     "start_time": "2024-11-06T05:50:28.774596Z"
    }
   },
   "source": [
    "# 导入必要的库\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# 加载 BERT 分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# 定义用于对输入文本进行分词的函数\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# 对数据集进行分词处理\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 从数据集中选择一小部分用于训练和测试\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=0).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=0).select(range(1000))\n",
    "\n",
    "# 加载 BERT-base-cased 模型用于序列分类任务\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
    "\n",
    "# 加载准确率度量\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# 定义用于计算评估指标的函数\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# 设置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10, \n",
    "    evaluation_strategy=\"epoch\")\n",
    "\n",
    "# 创建一个 Trainer 实例\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "trainer.train()\n",
    "\n",
    "# 将训练好的模型保存到磁盘上\n",
    "model.save_pretrained('./results/imdb_model')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 加载模型\n",
    "model = AutoModelForSequenceClassification.from_pretrained('./results/imdb_model')\n",
    "\n",
    "# 创建pipeline\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# 测试模型\n",
    "result = classifier('This is a great movie. I enjoyed it a lot!')\n",
    "print(result)\n",
    "\n",
    "# 测试模型\n",
    "result = classifier('This movie is so bad, I almost fell asleep.')\n",
    "print(result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
